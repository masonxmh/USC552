{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37de2e9f",
   "metadata": {},
   "source": [
    "<center><h1>DSCI-552 HOMEWORK 7</h1><center>\n",
    "<br>\n",
    "<center><font size=\"4\"></font></center>\n",
    "<center><font size=\"3\"><strong>Mason(Mohan) Xing</font></center>\n",
    "<center><font size=\"3\"><strong>USCID:\t6880083372</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95dbdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n",
      "Num GPUs Available:  1\n",
      "TensorFlow version: 2.7.0\n",
      "dsci552\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "# check if GPU is mounted\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "# check the environment\n",
    "print (os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e75c65",
   "metadata": {},
   "source": [
    "# 1. Generative Models for Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce5ba3",
   "metadata": {},
   "source": [
    "## (a) In this problem, we are trying to build a generative model to mimic the writing style of prominent British Mathematician, Philosopher, prolific writer, and political activist, Bertrand Russell.\n",
    "## (b) Download the following books from Project Gutenberg \n",
    "http://www.gutenberg.org/ebooks/author/355 in text format:\n",
    "## (c) LSTM: Train an LSTM to mimic Russell's style and thoughts:\n",
    "### 1(c) i. Concatenate your text files to create a corpus of Russell's writings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e799cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error file is: OKEWFSMP.txt\n",
      "Used data files are:  ['AIIMAT.txt', 'MLOE.txt', 'OKEWFSMP.txt', 'TAM.txt', 'TAMatter.txt', 'THWP.txt', 'TPP.txt']\n"
     ]
    }
   ],
   "source": [
    "# test if all txt file can be read\n",
    "for root, _, files in os.walk('../data/Book Files/books/'):\n",
    "    file_names_full = files\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(root + file, 'r') as f:\n",
    "                f.read()\n",
    "        except:\n",
    "            error_file = file\n",
    "            print(\"error file is:\", error_file)\n",
    "print(\"Used data files are: \", file_names_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a1f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt file based on different encoding method\n",
    "file_names = [x.rstrip('.txt') for x in file_names_full]\n",
    "book={}\n",
    "for root, _, files in os.walk('../data/Book Files/books/'):\n",
    "    for file in files:\n",
    "        if file == error_file:\n",
    "            with open(root + file, 'r', encoding=\"utf8\") as f:\n",
    "                book[file.rstrip('.txt')]=f.readlines()\n",
    "        else:\n",
    "            with open(root + file, 'r') as f:\n",
    "                book[file.rstrip('.txt')]=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e87c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ï»¿INTRODUCTION \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'The present work is intended as an investigation of certain \\n',\n",
       " 'problems concerning empirical knowledge. As opposed to tradi- \\n',\n",
       " 'tional theory of knowledge, the method adopted differs chiefly \\n',\n",
       " 'in the importance attached to linguistic considerations â€¢ I propose \\n',\n",
       " 'to consider language in relation to two main problems, which, in \\n',\n",
       " 'preliminary and not very precise terms, may be stated as follows : \\n',\n",
       " '\\n',\n",
       " '. What is meant by \"empirical evidence for the truth of a \\n',\n",
       " '\\n',\n",
       " 'proposition\" \\n',\n",
       " '\\n',\n",
       " 'II. What can be inferred from the fact that there sometimes is \\n',\n",
       " '\\n',\n",
       " 'such evidence .^ \\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original data sample display\n",
    "book['AIIMAT'][:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727094c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count upper case ratio each line\n",
    "def upper_count(str_obj):\n",
    "    upper_count = len(re.findall(r'[A-Z]',str_obj))\n",
    "    total_count = len(str_obj)\n",
    "    ratio = upper_count/total_count\n",
    "    return(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d479a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up txt\n",
    "corpus = \"\"\n",
    "for file_name in file_names:\n",
    "    temp_list = []\n",
    "    for line in book[file_name]:\n",
    "        # remove \"\\n\"\n",
    "        line1 = line.replace(\"- \\n\", \"\")\n",
    "        line2 = line1.replace(\"\\n\", \" \")\n",
    "        # remove non alphaneumaric characters\n",
    "        line3 = re.sub('[^a-zA-Z\\.\\,\\!\\?]',' ', line2)\n",
    "        # remove numeric values\n",
    "        line4 = re.sub(\"\\d+\", \"\", line3)\n",
    "        # remove title\n",
    "        line_uppercount = upper_count(line4)\n",
    "        if line_uppercount < 0.7:\n",
    "            corpus+=line4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bafe326c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The present work is intended as an investigation of certain problems concerning empirical knowledge. As opposed to traditional theory of knowledge, the method adopted differs chiefly in the importance attached to linguistic considerations I propose to consider language in relation to two main problems, which, in preliminary and not very precise terms, may be stated as follows . What is meant by empirical evidence for the truth of a proposition II. What can be inferred from the fact that there so'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove extra white space\n",
    "corpus_clean =\" \".join(corpus.split())\n",
    "corpus_clean[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773d934",
   "metadata": {},
   "source": [
    "### 1(c) ii. \n",
    "Use a character-level representation for this model by using extended ASCII that has N = 256 characters. \n",
    "Each character will be encoded into a an integer using its ASCII code. Rescale the integers to the range [0; 1], because LSTM uses a sigmoid activation function. LSTM will receive the rescaled integers as its input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8d6e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to convert character to ASCII code\n",
    "def to_ascii(text):\n",
    "    ascii_values = [ord(character) for character in text]\n",
    "    return ascii_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25483dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "# encoded into a an integer using its ASCII code\n",
    "corpus_lower = corpus_clean.lower()\n",
    "char_ascii = to_ascii(corpus_lower)\n",
    "ascii_array = np.array(char_ascii).reshape(-1,1)\n",
    "fitted_scaler = scaler.fit(ascii_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dc50997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to scale ASCII encoded characters\n",
    "def rescale(scaler, ascii_4trans):\n",
    "    '''\n",
    "    scaler: MinMaxScaler() after fitting\n",
    "    ascii_4trans : a list of ascii chars used for transforming\n",
    "    return : scaled ascii array\n",
    "    \n",
    "    '''\n",
    "    ascii_array_4trans = np.array(ascii_4trans).reshape(-1,1)\n",
    "    scale_ascii = scaler.transform(ascii_array_4trans)\n",
    "    \n",
    "    return scale_ascii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17e747",
   "metadata": {},
   "source": [
    "### 1(c) iii. & 1(c) iv.\n",
    "Choose a window size, e.g., W = 100.<br>\n",
    "Inputs to the network will be the first W-1 = 99 characters of each sequence, and the output of the network will be the Wth character of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95397e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate X_train, y_train\n",
    "def X_y_prepare(w, scaler, trans_ascii):\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    scale_char_ascii = rescale(fitted_scaler, trans_ascii)\n",
    "    for i in tqdm(range(len(scale_char_ascii)-w+1)):\n",
    "        \n",
    "        x_i = scale_char_ascii[i:i+w-1]\n",
    "        y_i = trans_ascii[i+w-1]\n",
    "        X_list.append(x_i)\n",
    "        y_list.append(y_i)\n",
    "        \n",
    "    X = np.array(X_list).reshape(len(X_list), w-1, 1)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa0bed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4919620/4919620 [00:02<00:00, 1774569.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4919620, 99, 1)\n",
      "(4919620,)\n"
     ]
    }
   ],
   "source": [
    "w = 100\n",
    "scaler = MinMaxScaler()\n",
    "# encoded into a an integer using its ASCII code\n",
    "corpus_lower = corpus_clean.lower()\n",
    "char_ascii = to_ascii(corpus_lower)\n",
    "X, y = X_y_prepare(w, scaler, char_ascii)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257d01d",
   "metadata": {},
   "source": [
    "### 1(c) v. \n",
    "Note that the output has to be encoded using a one-hot encoding scheme with N = 256 (or less) elements. This means that the network reads integers, but outputs a vector of N = 256 (or less) elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e3cae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4919620, 123)\n"
     ]
    }
   ],
   "source": [
    "y_encode = to_categorical(y)\n",
    "print(y_encode.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a346c9",
   "metadata": {},
   "source": [
    "### 1(c) vi & 1(c) vii. \n",
    "Use a single hidden layer for the LSTM with N = 256 (or less) memory units.<br>\n",
    "Use a Softmax output layer to yield a probability prediction for each of the characters between 0 and 1.<br>\n",
    "ref: https://www.analyticsvidhya.com/blog/2021/08/predict-the-next-word-of-your-text-using-long-short-term-memory-lstm/<br>\n",
    "ref: https://towardsdatascience.com/lstm-how-to-train-neural-networks-to-write-like-lovecraft-e56e1165f514<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f3b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 256)               264192    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 123)               31611     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 295,803\n",
      "Trainable params: 295,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(LSTM(256, input_shape=(X.shape[1], 1)))\n",
    "LSTM_model.add(Dropout(0.5))\n",
    "LSTM_model.add(Dense(y_encode.shape[1], activation='softmax'))\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f779d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "LSTM_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy','accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e92d3",
   "metadata": {},
   "source": [
    "### 1(c) viii & 1(c) ix & 1(c) x \n",
    "We do not use a test dataset. We are using the whole training dataset to learn the probability of each character in a sequence. We are not seeking for a very accurate model. Instead we are interested in a generalization of the dataset that can mimic the gist of the text..<br>\n",
    "Choose a reasonable number of epochs for training, considering your computational power (e.g., 30, although the network will need more epochs to yield a better model).<br>\n",
    "Use model checkpointing to keep the network weights to determine each time an improvement in loss is observed at the end of the epoch. Find the best set of weights in terms of loss.\n",
    "ref: https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae334f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../model/LSTM/dropout_0.5/\"\n",
    "for f in os.listdir(file_path):\n",
    "    os.remove(os.path.join(file_path, f))\n",
    "\n",
    "model_file=file_path + \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(model_file, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c8e71d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "38434/38435 [============================>.] - ETA: 0s - loss: 2.5152 - categorical_accuracy: 0.2711 - accuracy: 0.2711\n",
      "Epoch 00001: loss improved from inf to 2.51522, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-01-2.5152.hdf5\n",
      "38435/38435 [==============================] - 335s 9ms/step - loss: 2.5152 - categorical_accuracy: 0.2711 - accuracy: 0.2711\n",
      "Epoch 2/30\n",
      "38434/38435 [============================>.] - ETA: 0s - loss: 2.1633 - categorical_accuracy: 0.3705 - accuracy: 0.3705\n",
      "Epoch 00002: loss improved from 2.51522 to 2.16330, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-02-2.1633.hdf5\n",
      "38435/38435 [==============================] - 331s 9ms/step - loss: 2.1633 - categorical_accuracy: 0.3705 - accuracy: 0.3705\n",
      "Epoch 3/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: 2.0366 - categorical_accuracy: 0.4080 - accuracy: 0.4080\n",
      "Epoch 00003: loss improved from 2.16330 to 2.03661, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-03-2.0366.hdf5\n",
      "38435/38435 [==============================] - 330s 9ms/step - loss: 2.0366 - categorical_accuracy: 0.4080 - accuracy: 0.4080\n",
      "Epoch 4/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: 1.9703 - categorical_accuracy: 0.4275 - accuracy: 0.4275\n",
      "Epoch 00004: loss improved from 2.03661 to 1.97027, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-04-1.9703.hdf5\n",
      "38435/38435 [==============================] - 329s 9ms/step - loss: 1.9703 - categorical_accuracy: 0.4275 - accuracy: 0.4275\n",
      "Epoch 5/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: 1.9257 - categorical_accuracy: 0.4405 - accuracy: 0.4405\n",
      "Epoch 00005: loss improved from 1.97027 to 1.92570, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-05-1.9257.hdf5\n",
      "38435/38435 [==============================] - 323s 8ms/step - loss: 1.9257 - categorical_accuracy: 0.4405 - accuracy: 0.4405\n",
      "Epoch 6/30\n",
      "38435/38435 [==============================] - ETA: 0s - loss: 1.8929 - categorical_accuracy: 0.4501 - accuracy: 0.4501\n",
      "Epoch 00006: loss improved from 1.92570 to 1.89285, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-06-1.8929.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.8929 - categorical_accuracy: 0.4501 - accuracy: 0.4501\n",
      "Epoch 7/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: 1.8674 - categorical_accuracy: 0.4577 - accuracy: 0.4577\n",
      "Epoch 00007: loss improved from 1.89285 to 1.86735, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-07-1.8674.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.8674 - categorical_accuracy: 0.4577 - accuracy: 0.4577\n",
      "Epoch 8/30\n",
      "38435/38435 [==============================] - ETA: 0s - loss: 1.8464 - categorical_accuracy: 0.4640 - accuracy: 0.4640\n",
      "Epoch 00008: loss improved from 1.86735 to 1.84635, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-08-1.8464.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.8464 - categorical_accuracy: 0.4640 - accuracy: 0.4640\n",
      "Epoch 9/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: 1.8290 - categorical_accuracy: 0.4691 - accuracy: 0.4691\n",
      "Epoch 00009: loss improved from 1.84635 to 1.82900, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-09-1.8290.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.8290 - categorical_accuracy: 0.4691 - accuracy: 0.4691\n",
      "Epoch 10/30\n",
      "38435/38435 [==============================] - ETA: 0s - loss: 1.8149 - categorical_accuracy: 0.4728 - accuracy: 0.4728\n",
      "Epoch 00010: loss improved from 1.82900 to 1.81493, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-10-1.8149.hdf5\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.8149 - categorical_accuracy: 0.4728 - accuracy: 0.4728\n",
      "Epoch 11/30\n",
      "38435/38435 [==============================] - ETA: 0s - loss: 1.8014 - categorical_accuracy: 0.4769 - accuracy: 0.4769\n",
      "Epoch 00011: loss improved from 1.81493 to 1.80139, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-11-1.8014.hdf5\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.8014 - categorical_accuracy: 0.4769 - accuracy: 0.4769\n",
      "Epoch 12/30\n",
      "38431/38435 [============================>.] - ETA: 0s - loss: 1.7902 - categorical_accuracy: 0.4800 - accuracy: 0.4800\n",
      "Epoch 00012: loss improved from 1.80139 to 1.79024, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-12-1.7902.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7902 - categorical_accuracy: 0.4800 - accuracy: 0.4800\n",
      "Epoch 13/30\n",
      "38431/38435 [============================>.] - ETA: 0s - loss: 1.7797 - categorical_accuracy: 0.4829 - accuracy: 0.4829\n",
      "Epoch 00013: loss improved from 1.79024 to 1.77970, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-13-1.7797.hdf5\n",
      "38435/38435 [==============================] - 328s 9ms/step - loss: 1.7797 - categorical_accuracy: 0.4829 - accuracy: 0.4829\n",
      "Epoch 14/30\n",
      "38434/38435 [============================>.] - ETA: 0s - loss: 1.7713 - categorical_accuracy: 0.4854 - accuracy: 0.4854\n",
      "Epoch 00014: loss improved from 1.77970 to 1.77132, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-14-1.7713.hdf5\n",
      "38435/38435 [==============================] - 331s 9ms/step - loss: 1.7713 - categorical_accuracy: 0.4854 - accuracy: 0.4854\n",
      "Epoch 15/30\n",
      "38435/38435 [==============================] - ETA: 0s - loss: 1.7631 - categorical_accuracy: 0.4876 - accuracy: 0.4876\n",
      "Epoch 00015: loss improved from 1.77132 to 1.76307, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-15-1.7631.hdf5\n",
      "38435/38435 [==============================] - 331s 9ms/step - loss: 1.7631 - categorical_accuracy: 0.4876 - accuracy: 0.4876\n",
      "Epoch 16/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: 1.9422 - categorical_accuracy: 0.4376 - accuracy: 0.4376\n",
      "Epoch 00016: loss did not improve from 1.76307\n",
      "38435/38435 [==============================] - 330s 9ms/step - loss: 1.9422 - categorical_accuracy: 0.4376 - accuracy: 0.4376\n",
      "Epoch 17/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: 1.7529 - categorical_accuracy: 0.4908 - accuracy: 0.4908\n",
      "Epoch 00017: loss improved from 1.76307 to 1.75288, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-17-1.7529.hdf5\n",
      "38435/38435 [==============================] - 327s 8ms/step - loss: 1.7529 - categorical_accuracy: 0.4908 - accuracy: 0.4908\n",
      "Epoch 18/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: 1.7442 - categorical_accuracy: 0.4932 - accuracy: 0.4932\n",
      "Epoch 00018: loss improved from 1.75288 to 1.74423, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-18-1.7442.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7442 - categorical_accuracy: 0.4932 - accuracy: 0.4932\n",
      "Epoch 19/30\n",
      "38429/38435 [============================>.] - ETA: 0s - loss: 1.7380 - categorical_accuracy: 0.4948 - accuracy: 0.4948\n",
      "Epoch 00019: loss improved from 1.74423 to 1.73804, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-19-1.7380.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7380 - categorical_accuracy: 0.4948 - accuracy: 0.4948\n",
      "Epoch 20/30\n",
      "38430/38435 [============================>.] - ETA: 0s - loss: 1.7322 - categorical_accuracy: 0.4962 - accuracy: 0.4962\n",
      "Epoch 00020: loss improved from 1.73804 to 1.73216, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-20-1.7322.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7322 - categorical_accuracy: 0.4962 - accuracy: 0.4962\n",
      "Epoch 21/30\n",
      "38431/38435 [============================>.] - ETA: 0s - loss: 1.7275 - categorical_accuracy: 0.4978 - accuracy: 0.4978\n",
      "Epoch 00021: loss improved from 1.73216 to 1.72746, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-21-1.7275.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7275 - categorical_accuracy: 0.4978 - accuracy: 0.4978\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38435/38435 [==============================] - ETA: 0s - loss: 1.7226 - categorical_accuracy: 0.4991 - accuracy: 0.4991\n",
      "Epoch 00022: loss improved from 1.72746 to 1.72264, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-22-1.7226.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7226 - categorical_accuracy: 0.4991 - accuracy: 0.4991\n",
      "Epoch 23/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: 1.7180 - categorical_accuracy: 0.5003 - accuracy: 0.5003\n",
      "Epoch 00023: loss improved from 1.72264 to 1.71803, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-23-1.7180.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7180 - categorical_accuracy: 0.5003 - accuracy: 0.5003\n",
      "Epoch 24/30\n",
      "38434/38435 [============================>.] - ETA: 0s - loss: 1.7138 - categorical_accuracy: 0.5015 - accuracy: 0.5015\n",
      "Epoch 00024: loss improved from 1.71803 to 1.71377, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-24-1.7138.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7138 - categorical_accuracy: 0.5015 - accuracy: 0.5015\n",
      "Epoch 25/30\n",
      "38434/38435 [============================>.] - ETA: 0s - loss: 1.7095 - categorical_accuracy: 0.5027 - accuracy: 0.5027\n",
      "Epoch 00025: loss improved from 1.71377 to 1.70951, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-25-1.7095.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7095 - categorical_accuracy: 0.5027 - accuracy: 0.5027\n",
      "Epoch 26/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: 1.7056 - categorical_accuracy: 0.5039 - accuracy: 0.5039\n",
      "Epoch 00026: loss improved from 1.70951 to 1.70563, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-26-1.7056.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7056 - categorical_accuracy: 0.5039 - accuracy: 0.5039\n",
      "Epoch 27/30\n",
      "38435/38435 [==============================] - ETA: 0s - loss: 1.7019 - categorical_accuracy: 0.5050 - accuracy: 0.5050\n",
      "Epoch 00027: loss improved from 1.70563 to 1.70190, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-27-1.7019.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.7019 - categorical_accuracy: 0.5050 - accuracy: 0.5050\n",
      "Epoch 28/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: 1.6983 - categorical_accuracy: 0.5059 - accuracy: 0.5059\n",
      "Epoch 00028: loss improved from 1.70190 to 1.69830, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-28-1.6983.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.6983 - categorical_accuracy: 0.5059 - accuracy: 0.5059\n",
      "Epoch 29/30\n",
      "38434/38435 [============================>.] - ETA: 0s - loss: 1.6942 - categorical_accuracy: 0.5072 - accuracy: 0.5072\n",
      "Epoch 00029: loss improved from 1.69830 to 1.69416, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-29-1.6942.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.6942 - categorical_accuracy: 0.5072 - accuracy: 0.5072\n",
      "Epoch 30/30\n",
      "38431/38435 [============================>.] - ETA: 0s - loss: 1.6913 - categorical_accuracy: 0.5080 - accuracy: 0.5080\n",
      "Epoch 00030: loss improved from 1.69416 to 1.69128, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-30-1.6913.hdf5\n",
      "38435/38435 [==============================] - 324s 8ms/step - loss: 1.6913 - categorical_accuracy: 0.5080 - accuracy: 0.5080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d2801ca8e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "LSTM_model.fit(X, y_encode, epochs=30 , batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0006e8",
   "metadata": {},
   "source": [
    "### 1(c)xi. Use the network with the best weights to generate 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff3ecbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_newtext(model, text, w, scale):\n",
    "    \n",
    "    for i in tqdm(range(1000)):\n",
    "    \n",
    "        text_lower = text.lower()\n",
    "        text_ascii = to_ascii(text_lower)\n",
    "        seq = text_ascii[-(w-1):]\n",
    "        seq_scale = rescale(fitted_scaler, seq)\n",
    "        seq_scale_arr = np.array(seq_scale)\n",
    "        X_test =  seq_scale_arr.reshape(1,w-1,1)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_ascii = np.argmax(y_pred, axis=None, out=None)\n",
    "        y_pred_char = chr(y_pred_ascii)\n",
    "        text+=y_pred_char\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b13495ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_text = \"There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16f8ae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 29.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object. the soatate seannd seannd aelieting the soace of the soace of the soace of the soace of the soace of the soace thmng that iave the semse data of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data and the soace of the soace thmng that iave the semse data an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loading the saved model\n",
    "folder_path = file_path\n",
    "file_type = '\\*.hdf5'\n",
    "files = glob.glob(folder_path + file_type)\n",
    "max_file = max(files, key=os.path.getctime)\n",
    "\n",
    "loaded_model = load_model(max_file)\n",
    "test_text = original_test_text + ''\n",
    "w = 100\n",
    "\n",
    "new_text = generate_newtext(loaded_model, test_text, w, fitted_scaler)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af42f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: 1.6880 - categorical_accuracy: 0.5089 - accuracy: 0.5089\n",
      "Epoch 00001: loss improved from 1.69128 to 1.68796, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-01-1.6880.hdf5\n",
      "38435/38435 [==============================] - 326s 8ms/step - loss: 1.6880 - categorical_accuracy: 0.5089 - accuracy: 0.5089\n",
      "Epoch 2/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: 1.6837 - categorical_accuracy: 0.5098 - accuracy: 0.5098\n",
      "Epoch 00002: loss improved from 1.68796 to 1.68366, saving model to ../model/LSTM/dropout_0.5\\weights-improvement-02-1.6837.hdf5\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.6837 - categorical_accuracy: 0.5098 - accuracy: 0.5098\n",
      "Epoch 3/30\n",
      "38431/38435 [============================>.] - ETA: 0s - loss: 1.7795 - categorical_accuracy: 0.4844 - accuracy: 0.4844\n",
      "Epoch 00003: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 326s 8ms/step - loss: 1.7796 - categorical_accuracy: 0.4843 - accuracy: 0.4843\n",
      "Epoch 4/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: 2.3803 - categorical_accuracy: 0.3076 - accuracy: 0.3076\n",
      "Epoch 00004: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 2.3803 - categorical_accuracy: 0.3076 - accuracy: 0.3076\n",
      "Epoch 5/30\n",
      "38435/38435 [==============================] - ETA: 0s - loss: 1.9299 - categorical_accuracy: 0.4394 - accuracy: 0.4394\n",
      "Epoch 00005: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.9299 - categorical_accuracy: 0.4394 - accuracy: 0.4394\n",
      "Epoch 6/30\n",
      "38430/38435 [============================>.] - ETA: 0s - loss: 1.8376 - categorical_accuracy: 0.4659 - accuracy: 0.4659\n",
      "Epoch 00006: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.8376 - categorical_accuracy: 0.4659 - accuracy: 0.4659\n",
      "Epoch 7/30\n",
      "38430/38435 [============================>.] - ETA: 0s - loss: 1.7951 - categorical_accuracy: 0.4781 - accuracy: 0.4781\n",
      "Epoch 00007: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.7951 - categorical_accuracy: 0.4781 - accuracy: 0.4781\n",
      "Epoch 8/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: 1.7677 - categorical_accuracy: 0.4859 - accuracy: 0.4859\n",
      "Epoch 00008: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.7677 - categorical_accuracy: 0.4859 - accuracy: 0.4859\n",
      "Epoch 9/30\n",
      "38431/38435 [============================>.] - ETA: 0s - loss: 1.8121 - categorical_accuracy: 0.4729 - accuracy: 0.4729\n",
      "Epoch 00009: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.8121 - categorical_accuracy: 0.4729 - accuracy: 0.4729\n",
      "Epoch 10/30\n",
      "38434/38435 [============================>.] - ETA: 0s - loss: 1.8288 - categorical_accuracy: 0.4674 - accuracy: 0.4674\n",
      "Epoch 00010: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.8288 - categorical_accuracy: 0.4674 - accuracy: 0.4674\n",
      "Epoch 11/30\n",
      "38435/38435 [==============================] - ETA: 0s - loss: 1.7599 - categorical_accuracy: 0.4874 - accuracy: 0.4874\n",
      "Epoch 00011: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.7599 - categorical_accuracy: 0.4874 - accuracy: 0.4874\n",
      "Epoch 12/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: 1.7341 - categorical_accuracy: 0.4944 - accuracy: 0.4944\n",
      "Epoch 00012: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.7342 - categorical_accuracy: 0.4944 - accuracy: 0.4944\n",
      "Epoch 13/30\n",
      "38430/38435 [============================>.] - ETA: 0s - loss: 1.7585 - categorical_accuracy: 0.4878 - accuracy: 0.4878\n",
      "Epoch 00013: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 1.7585 - categorical_accuracy: 0.4878 - accuracy: 0.4878\n",
      "Epoch 14/30\n",
      "38430/38435 [============================>.] - ETA: 0s - loss: 2.0430 - categorical_accuracy: 0.4074 - accuracy: 0.4074\n",
      "Epoch 00014: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 2.0429 - categorical_accuracy: 0.4074 - accuracy: 0.4074\n",
      "Epoch 15/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: 2.4958 - categorical_accuracy: 0.2916 - accuracy: 0.2916\n",
      "Epoch 00015: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 2.4958 - categorical_accuracy: 0.2916 - accuracy: 0.2916\n",
      "Epoch 16/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: 2.7128 - categorical_accuracy: 0.2228 - accuracy: 0.2228\n",
      "Epoch 00016: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 2.7128 - categorical_accuracy: 0.2228 - accuracy: 0.2228\n",
      "Epoch 17/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: 2.6910 - categorical_accuracy: 0.2270 - accuracy: 0.2270\n",
      "Epoch 00017: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: 2.6911 - categorical_accuracy: 0.2270 - accuracy: 0.2270\n",
      "Epoch 18/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0831 - accuracy: 0.0831\n",
      "Epoch 00018: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0831 - accuracy: 0.0831\n",
      "Epoch 19/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00019: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00020: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00021: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "38431/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00022: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00023: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "38430/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00024: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "38430/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00025: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "38431/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00026: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38431/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00027: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "38432/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00028: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "38433/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00029: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "38434/38435 [============================>.] - ETA: 0s - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "Epoch 00030: loss did not improve from 1.68366\n",
      "38435/38435 [==============================] - 325s 8ms/step - loss: nan - categorical_accuracy: 0.0000e+00 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d2c671c700>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model for 30 epoch more\n",
    "loaded_model.fit(X, y_encode, epochs=30, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a00f3865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:33<00:00, 29.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object. the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of the soace of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loading the saved model\n",
    "folder_path = file_path\n",
    "file_type = '\\*.hdf5'\n",
    "files = glob.glob(folder_path + file_type)\n",
    "max_file = max(files, key=os.path.getctime)\n",
    "\n",
    "loaded_model = load_model(max_file)\n",
    "test_text = original_test_text + ''\n",
    "w = 100\n",
    "\n",
    "new_text = generate_newtext(loaded_model, test_text, w, fitted_scaler)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91362f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796cd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f416ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
